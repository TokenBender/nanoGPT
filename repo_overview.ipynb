{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# nanoGPT Repository Overview\nThis notebook provides an overview of the key components of the nanoGPT repository.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Repository Structure\n- `train.py`: Main training script.\n- `model.py`: Definition of the GPT model.\n- `config/`: configuration files for different experiments.\n- `sample.py`: script to generate text from a trained model.\n- `data/`: data preparation scripts for various datasets.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Preparation\nDatasets are stored in the `data/` folder. Each dataset directory usually contains a `prepare.py` script that downloads the data and creates `train.bin` and `val.bin` files of tokenized integers. Example:`python data/shakespeare_char/prepare.py`."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!ls -R data | head"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Configuration Files\nExperiments are driven by config files in the `config/` directory. Each config is a Python file that sets hyperparameters used by `train.py`. You can pass the path to a config file when launching training:"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!head -n 20 config/train_shakespeare_char.py"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Model Definition (`model.py`)\nThe GPT model is implemented in about 300 lines. Below we print the first 20 lines:"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!head -n 20 model.py"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training Script (`train.py`)\nThe training loop is contained in `train.py`. It supports single-GPU and distributed training. Below we look at its first 20 lines:"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!head -n 20 train.py"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Sampling Text\nOnce a model is trained, `sample.py` can be used to generate text.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!head -n 20 sample.py"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Running a Minimal Example\nBelow we run the Shakespeare character dataset preparation and start a small training run. On a CPU this trains a tiny model for only a few iterations for demonstration."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!python data/shakespeare_char/prepare.py && python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=1 --log_interval=1 --block_size=64 --batch_size=4 --n_layer=2 --n_head=2 --n_embd=64 --max_iters=2 --lr_decay_iters=2 --dropout=0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conclusion\nThis notebook walked through the main parts of nanoGPT: data preparation, configuration, model implementation, training, and sampling. Use the code cells above as starting points to explore the repository further."
    }
  ]
}
