{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Tiny Shakespeare Dataset (Character-Level) for nanoGPT\n",
    "\n",
    "This notebook walks through the `prepare.py` script located in `data/shakespeare_char/`. This script processes the Tiny Shakespeare dataset for training nanoGPT models using character-level encoding, where each character is treated as a unique token. We'll integrate explanations from the provided documentation to offer a comprehensive understanding of each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "### Purpose and Scope (from documentation)\n",
    "The `prepare.py` script for the character-level Shakespeare dataset transforms the raw text of Shakespeare's works into tokenized binary files. Unlike other preparation scripts that might use BPE tokenization (like GPT-2's), this one maps each character directly to an integer. These files (`train.bin`, `val.bin`, and `meta.pkl`) are then used for training character-based language models.\n",
    "\n",
    "### Overview of Character-Level Preparation (from documentation)\n",
    "Data preparation in nanoGPT, as exemplified by this script, converts raw text into arrays of integer token IDs. For character-level models, the 'tokens' are individual characters. \n",
    "Key characteristics of this preparation (from documentation):\n",
    "*   Vocabulary size: 65 (based on unique characters in the Tiny Shakespeare dataset).\n",
    "*   Simple integer mapping for each character.\n",
    "*   Metadata (character-to-ID mapping, vocabulary size) saved in `meta.pkl` for encoding/decoding during generation.\n",
    "*   A 90/10 train/validation split is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "# script_dir = os.getcwd() # Original line\n",
    "# In a notebook, __file__ is not defined. We'll define script_dir to be the current working directory,\n",
    "# which should be 'data/shakespeare_char/' if you are running this notebook from there as intended.\n",
    "script_dir = '.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading the Dataset\n",
    "\n",
    "### The Tiny Shakespeare Dataset (from script)\n",
    "The script begins by downloading the Tiny Shakespeare dataset if it's not already present locally as `input.txt`. This is the same raw text file used by the BPE token-level Shakespeare preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = os.path.join(script_dir, 'input.txt') # Use script_dir defined earlier\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f: # Added encoding for consistency with .py\n",
    "        f.write(requests.get(data_url).text)\n",
    "    print(f\"Downloaded and saved dataset to {input_file_path}\")\n",
    "else:\n",
    "    print(f\"Dataset {input_file_path} already exists.\")\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f: # Added encoding for consistency with .py\n",
    "    data = f.read()\n",
    "print(f\"Length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character Vocabulary Creation and Encoding\n",
    "\n",
    "### Building the Vocabulary (from script & documentation)\n",
    "This is the core of character-level tokenization:\n",
    "1.  The set of all unique characters in the dataset is extracted.\n",
    "2.  This set is sorted to ensure consistent mapping.\n",
    "3.  The vocabulary size is determined by the number of unique characters.\n",
    "4.  Two dictionaries are created:\n",
    "    *   `stoi` (string-to-integer): Maps each character to a unique integer ID.\n",
    "    *   `itos` (integer-to-string): Maps each integer ID back to its character.\n",
    "\n",
    "The documentation confirms the vocabulary size for this dataset is 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All the unique characters:\", ''.join(chars))\n",
    "print(f\"Vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Example encoding/decoding\n",
    "print(\"Example encoding of 'hello':\", encode('hello'))\n",
    "print(\"Example decoding of [46, 43, 50, 50, 53]:\", decode([46, 43, 50, 50, 53]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Encoding (from script)\n",
    "The dataset is split into training (90%) and validation (10%) sets. The `encode` function is then used to convert the raw character strings of these splits into lists of integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens (characters)\")\n",
    "print(f\"val has {len(val_ids):,} tokens (characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exporting to Binary Files and Saving Metadata\n",
    "\n",
    "### Data Format and Storage (from documentation)\n",
    "The integer ID sequences are stored in binary files:\n",
    "*   Training data: `train.bin`\n",
    "*   Validation data: `val.bin`\n",
    "*   Data type: `np.uint16`. This is suitable as the vocabulary size (65) is much smaller than 2^16.\n",
    "\n",
    "The documentation table for Shakespeare (char) indicates:\n",
    "*   Training Tokens: ~1M\n",
    "*   Validation Tokens: ~111K\n",
    "*   Format: Character-level\n",
    "\n",
    "### Exporting Binary Files (from script)\n",
    "The lists of token IDs are converted to NumPy arrays and saved to `.bin` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(script_dir, 'train.bin')) # Use script_dir\n",
    "val_ids.tofile(os.path.join(script_dir, 'val.bin'))   # Use script_dir\n",
    "print(f\"Finished writing train.bin and val.bin to {script_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Metadata (from script & documentation)\n",
    "A crucial step for character-level models is saving the vocabulary and encoding/decoding mappings. This is stored in `meta.pkl` using Python's `pickle` module.\n",
    "The metadata includes:\n",
    "*   `vocab_size`: The number of unique characters.\n",
    "*   `itos`: The integer-to-string mapping.\n",
    "*   `stoi`: The string-to-integer mapping.\n",
    "\n",
    "This `meta.pkl` file is essential later for generating text from a trained character-level model, as it allows converting the model's output (integer IDs) back into readable characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(script_dir, 'meta.pkl'), 'wb') as f: # Use script_dir\n",
    "    pickle.dump(meta, f)\n",
    "print(f\"Saved metadata to {os.path.join(script_dir, 'meta.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Output and Statistics\n",
    "\n",
    "The script produces three files in the `data/shakespeare_char/` directory:\n",
    "* `train.bin`: Character IDs for the training data (approx. 1 million tokens).\n",
    "* `val.bin`: Character IDs for the validation data (approx. 111,000 tokens).\n",
    "* `meta.pkl`: Contains the vocabulary size and the character-to-integer mappings.\n",
    "\n",
    "These files are now ready for training a character-level language model with nanoGPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
