{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Tiny Shakespeare Dataset (Character-Level)\n",
    "\n",
    "Welcome! This notebook walks you through the process of preparing the Tiny Shakespeare dataset for training a character-level language model with nanoGPT. \n",
    "\n",
    "**What is Character-Level Tokenization?**\n",
    "\n",
    "In Natural Language Processing (NLP), tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, sub-words, or, in this case, individual characters.\n",
    "\n",
    "For example, the sentence \"Hello, world!\" would be tokenized at the character level as: `['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']`.\n",
    "\n",
    "**Why Use Character-Level Tokenization?**\n",
    "\n",
    "Character-level models have a few advantages:\n",
    "*   **Smaller Vocabulary:** The vocabulary (the set of unique tokens) is much smaller, consisting only of the unique characters in the text (letters, numbers, punctuation, spaces, etc.). This means the model needs fewer embeddings.\n",
    "*   **Handles Out-of-Vocabulary (OOV) Words:** Word-level models can struggle with words not seen during training. Character models inherently handle any sequence of characters.\n",
    "*   **Can Model Sub-Word Information:** They can potentially learn patterns within words (like prefixes and suffixes).\n",
    "\n",
    "However, they also have disadvantages:\n",
    "*   **Longer Sequences:** Representing text requires much longer sequences of tokens compared to word-level models, which can make it harder for the model to learn long-range dependencies.\n",
    "*   **Less Semantic Meaning per Token:** Individual characters carry less semantic meaning than whole words.\n",
    "\n",
    "This notebook will guide you through downloading the data, creating a character vocabulary, encoding the text into numerical format, and saving it in a way that's ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "First, let's import the necessary Python libraries:\n",
    "\n",
    "*   `os`: This module provides a way of using operating system dependent functionality like reading or writing to the file system. We'll use it for path manipulations and checking if files exist.\n",
    "*   `pickle`: This module is used for serializing and de-serializing Python object structures, also known as \"pickling\" and \"unpickling\". We'll use it to save our vocabulary metadata.\n",
    "*   `requests`: This library allows us to send HTTP requests. We'll use it to download the dataset if it's not already present locally.\n",
    "*   `numpy`: NumPy is a fundamental package for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. We'll use it to efficiently store our tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Script Directory\n",
    "\n",
    "We define `script_dir` using `os.getcwd()`. This assumes you are running this notebook from its location within the `data/shakespeare_char/` directory. All output files (`input.txt`, `train.bin`, `val.bin`, `meta.pkl`) will be saved relative to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "# For consistency with the original prepare.py, we could also use:\n",
    "# script_dir = os.path.dirname(__file__) \n",
    "# However, __file__ is not defined in interactive notebook environments by default.\n",
    "# os.getcwd() works well if the notebook is run from its directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download the Dataset\n",
    "\n",
    "Next, we'll download the Tiny Shakespeare dataset. This dataset consists of a collection of Shakespeare's works concatenated into a single text file.\n",
    "\n",
    "The code first defines the `input_file_path` where the data will be saved (`input.txt` in the `script_dir`).\n",
    "It then checks if this file already exists. If it doesn't, the code downloads the data from the specified URL (on `raw.githubusercontent.com`, provided by Andrej Karpathy's char-rnn project) and saves it to `input_file_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = os.path.join(script_dir, 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    print(f\"Downloading dataset from {data_url}...\")\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "    print(\"Dataset downloaded.\")\n",
    "else:\n",
    "    print(\"Dataset already exists locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Inspect the Data\n",
    "\n",
    "Now that we have the `input.txt` file, we'll read its content into a string variable called `data`.\n",
    "We then print the total length of the dataset (number of characters) to get an idea of its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "print(f\"Length of dataset in characters: {len(data):,}\")\n",
    "print(f\"First 100 characters: {data[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Character Vocabulary\n",
    "\n",
    "This is a crucial step in preparing data for a character-level language model.\n",
    "\n",
    "**Why a Vocabulary?**\n",
    "Language models work with numbers, not raw text. We need a way to convert each character in our dataset into a unique numerical representation (an integer ID). A vocabulary defines this mapping.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Extract Unique Characters:** We first find all the unique characters present in the `data`. The `set(data)` operation creates a collection of unique characters, and `sorted(list(...))` converts this set into a sorted list. Sorting ensures that our vocabulary is consistent every time we run this script.\n",
    "2.  **Vocabulary Size:** The number of unique characters determines our `vocab_size`.\n",
    "3.  **String-to-Integer (stoi) Mapping:** We create a dictionary called `stoi` where keys are characters and values are their corresponding integer IDs (from 0 to `vocab_size - 1`). For example, if `chars = ['a', 'b', 'c']`, then `stoi` would be `{'a':0, 'b':1, 'c':2}`.\n",
    "4.  **Integer-to-String (itos) Mapping:** We also create the reverse mapping, `itos`, where keys are integer IDs and values are the corresponding characters. For the example above, `itos` would be `{0:'a', 1:'b', 2:'c'}`. This is useful for decoding the model's output back into readable text.\n",
    "\n",
    "We then define two helper functions:\n",
    "*   `encode(s)`: Takes a string `s` and returns a list of integers representing that string according to the `stoi` mapping.\n",
    "*   `decode(l)`: Takes a list of integers `l` and returns the corresponding string using the `itos` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All the unique characters:\", ''.join(chars))\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "\n",
    "# Create a mapping from characters to integers and vice-versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Encoder: take a string, output a list of integers\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] \n",
    "\n",
    "# Decoder: take a list of integers, output a string\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Example of encoding and decoding\n",
    "test_string = \"hello world\"\n",
    "encoded_string = encode(test_string)\n",
    "decoded_string = decode(encoded_string)\n",
    "print(f\"Original string: {test_string}\")\n",
    "print(f\"Encoded string: {encoded_string}\")\n",
    "print(f\"Decoded string: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Train and Validation Splits\n",
    "\n",
    "**Why Split Data?**\n",
    "It's standard practice in machine learning to split your dataset into at least two parts:\n",
    "*   **Training Set:** This is the data the model learns from. The model's parameters are adjusted based on its performance on this set.\n",
    "*   **Validation Set:** This data is held out and not used during the training phase. Instead, it's used to evaluate the model's performance on unseen data, helping to tune hyperparameters and check for overfitting (where the model performs well on training data but poorly on new data).\n",
    "\n",
    "Here, we split the dataset such that the first 90% of the text forms the training data (`train_data`) and the remaining 10% forms the validation data (`val_data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Length of training data: {len(train_data):,} characters\")\n",
    "print(f\"Length of validation data: {len(val_data):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode Data Splits\n",
    "\n",
    "Now that we have our training and validation text data, we use the `encode` function (defined earlier) to convert both splits from strings of characters into sequences of integer IDs.\n",
    "\n",
    "The resulting `train_ids` and `val_ids` will be lists of integers, ready to be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train_ids has {len(train_ids):,} tokens (integers)\")\n",
    "print(f\"val_ids has {len(val_ids):,} tokens (integers)\")\n",
    "print(f\"First 10 tokens of train_ids: {train_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to Binary Files\n",
    "\n",
    "For efficient loading during training, especially with large datasets, it's common to save the tokenized data in a compact binary format.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Convert to NumPy Arrays:** We first convert our lists of token IDs (`train_ids` and `val_ids`) into NumPy arrays.\n",
    "2.  **Specify Data Type (`dtype`):** We use `dtype=np.uint16`. This means each token ID will be stored as an unsigned 16-bit integer. \n",
    "    *   `unsigned` means it can only store non-negative numbers.\n",
    "    *   `16-bit` means it can store values from 0 to 2<sup>16</sup> - 1 (which is 0 to 65,535).\n",
    "    Since our `vocab_size` (65 for Shakespeare) is much smaller than 65,535, `np.uint16` is a suitable and memory-efficient choice. If the vocabulary were larger (e.g., > 65,535), we might need `np.uint32`.\n",
    "3.  **Save to File:** The `.tofile()` method of a NumPy array writes the array's data to a binary file. This results in `train.bin` and `val.bin`.\n",
    "\n",
    "These `.bin` files will contain a flat sequence of these 16-bit integers, which can be quickly read and loaded into memory by the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(os.path.join(script_dir, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(script_dir, 'val.bin'))\n",
    "\n",
    "print(\"train.bin and val.bin saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Metadata\n",
    "\n",
    "Finally, we need to save the information required to interpret our processed data later. This includes:\n",
    "\n",
    "*   `vocab_size`: The total number of unique characters.\n",
    "*   `itos`: The integer-to-string mapping (dictionary).\n",
    "*   `stoi`: The string-to-integer mapping (dictionary).\n",
    "\n",
    "This metadata is crucial because:\n",
    "*   The training script might need `vocab_size` to define the model architecture (e.g., the size of the embedding layer).\n",
    "*   When we want to generate text from the trained model, it will output sequences of integer IDs. We'll need `itos` to decode these IDs back into human-readable characters.\n",
    "*   If we want to feed new, unseen text to the model (e.g., for a prompt), we'll need `stoi` to tokenize that text.\n",
    "\n",
    "We store this information in a Python dictionary called `meta` and then use the `pickle` library to serialize this dictionary and save it to a file named `meta.pkl`.\n",
    "`pickle.dump(object_to_save, file_handle, protocol)` writes the pickled representation of `object_to_save` to the open `file_handle`. `'wb'` mode means we're writing in binary mode, which is required by pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(script_dir, 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)\n",
    "\n",
    "print(\"meta.pkl saved.\")\n",
    "\n",
    "# To load this metadata later, you would use:\n",
    "# with open('meta.pkl', 'rb') as f:\n",
    "#     meta = pickle.load(f)\n",
    "# print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "That's it! We have successfully processed the Tiny Shakespeare dataset for character-level language modeling.\n",
    "\n",
    "As a result of running this notebook, you should now have the following files in your `data/shakespeare_char/` directory (or wherever `script_dir` was pointing):\n",
    "\n",
    "*   `input.txt`: The raw dataset.\n",
    "*   `train.bin`: The training data, tokenized and stored as a binary sequence of 16-bit integers.\n",
    "*   `val.bin`: The validation data, similarly processed.\n",
    "*   `meta.pkl`: A pickle file containing the vocabulary size and the character-to-integer (`stoi`) and integer-to-character (`itos`) mappings.\n",
    "\n",
    "These files are now ready to be used for training a character-level nanoGPT model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
