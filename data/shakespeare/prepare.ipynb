{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Tiny Shakespeare Dataset (Token-Level) for nanoGPT\n",
    "\n",
    "This notebook walks through the `prepare.py` script located in `data/shakespeare/`, which processes the Tiny Shakespeare dataset for training nanoGPT models using token-level encoding (GPT-2 BPE). We'll integrate explanations from the provided documentation to offer a comprehensive understanding of each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "### Purpose and Scope (from documentation)\n",
    "The `prepare.py` script for the Shakespeare dataset transforms the raw text of Shakespeare's works into tokenized binary files. This specific version uses the standard GPT-2 BPE tokenizer. These files (`train.bin` and `val.bin`) can be efficiently loaded during model training and are part of nanoGPT's overall data preparation strategy.\n",
    "\n",
    "### Overview of Data Preparation (from documentation)\n",
    "Data preparation in nanoGPT, as exemplified by this script, converts raw text into arrays of integer token IDs. These are then stored in binary files. This method allows for efficient memory-mapping during training, meaning the entire dataset doesn't need to fit in RAM. The Shakespeare token-level dataset is one of the smaller datasets provided as an example with nanoGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "# script_dir = os.getcwd() # Original line\n",
    "# In a notebook, __file__ is not defined. We'll define script_dir to be the current working directory,\n",
    "# which should be 'data/shakespeare/' if you are running this notebook from there as intended.\n",
    "script_dir = '.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading and Loading the Dataset\n",
    "\n",
    "### The Tiny Shakespeare Dataset (from documentation & script)\n",
    "This script uses the \"Tiny Shakespeare\" dataset, a collection of Shakespeare's works. It's a relatively small dataset, making it good for quick experimentation.\n",
    "\n",
    "The first step is to download the dataset if it's not already present in the `input.txt` file. The data is fetched from a URL pointing to a plain text file on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = os.path.join(script_dir, 'input.txt') # Use script_dir defined earlier\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "    print(f\"Downloaded and saved dataset to {input_file_path}\")\n",
    "else:\n",
    "    print(f\"Dataset {input_file_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting and Tokenization\n",
    "\n",
    "### Splitting the Data (from script)\n",
    "The loaded text data is split into training and validation sets. 90% of the data is used for training, and the remaining 10% is used for validation.\n",
    "\n",
    "### Tokenization Process (from documentation and script)\n",
    "The script uses the standard GPT-2 Byte Pair Encoding (BPE) tokenizer from the `tiktoken` library. \n",
    "The `enc.encode_ordinary()` function converts the text of the train and validation sets into sequences of token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Format, Storage, and Exporting to Binary Files\n",
    "\n",
    "### Data Format and Storage (from documentation)\n",
    "As with other nanoGPT datasets, the tokenized Shakespeare data is stored as arrays of integer token IDs in binary files:\n",
    "* Training data: `train.bin`\n",
    "* Validation data: `val.bin`\n",
    "* Data type: `np.uint16` (since GPT-2's max token ID is < 2^16).\n",
    "\n",
    "The documentation table for Shakespeare (word) indicates:\n",
    "* Training Tokens: ~302K\n",
    "* Validation Tokens: ~36K\n",
    "* Format: GPT-2 BPE tokens\n",
    "\n",
    "These binary files are memory-mapped during training for efficient data access.\n",
    "\n",
    "### Exporting to Files (from script)\n",
    "The token ID lists (`train_ids` and `val_ids`) are converted to NumPy arrays with `dtype=np.uint16`. These arrays are then written directly to `.bin` files using the `tofile()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(script_dir, 'train.bin')) # Use script_dir\n",
    "val_ids.tofile(os.path.join(script_dir, 'val.bin'))   # Use script_dir\n",
    "print(f\"Finished writing train.bin and val.bin to {script_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Output and Statistics\n",
    "\n",
    "The script produces two binary files in the current directory (`data/shakespeare/`):\n",
    "* `train.bin`: Contains the token IDs for the training data.\n",
    "* `val.bin`: Contains the token IDs for the validation data.\n",
    "\n",
    "The script output indicates approximately 301,966 tokens for `train.bin` and 36,059 tokens for `val.bin`, which aligns with the documentation.\n",
    "\n",
    "These files are ready for use with nanoGPT's training script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
