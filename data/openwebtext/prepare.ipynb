{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the OpenWebText Dataset for nanoGPT\n",
    "\n",
    "This notebook walks through the `prepare.py` script, which processes the OpenWebText dataset for training nanoGPT models. We'll integrate explanations from the provided documentation to offer a comprehensive understanding of each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "### Purpose and Scope (from documentation)\n",
    "The `prepare.py` script transforms raw text data from the OpenWebText dataset into tokenized binary files. These files can be efficiently loaded during model training. This process is part of nanoGPT's data preparation pipeline, which is crucial for handling large datasets.\n",
    "\n",
    "### Overview of Data Preparation (from documentation)\n",
    "Data preparation in nanoGPT, as exemplified by this script, converts raw text into arrays of integer token IDs. These are then stored in binary files (`train.bin` and `val.bin`). This method allows for efficient memory-mapping during training, meaning the entire dataset doesn't need to fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "# script_dir = os.getcwd() # original line, but os.path.dirname(__file__) is used in .py\n",
    "# In a notebook, __file__ is not defined. We'll define script_dir to be the current working directory, \n",
    "# which should be 'data/openwebtext/' if you are running this notebook from there as intended.\n",
    "script_dir = '.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Splitting the Dataset\n",
    "\n",
    "### The OpenWebText Dataset (from documentation)\n",
    "The OpenWebText dataset is a large corpus of text sourced from the web, similar to the dataset used for training the original GPT-2 model. It contains approximately 8 million documents.\n",
    "\n",
    "The first step in `prepare.py` is to download (if not already cached by HuggingFace `datasets`) and load the OpenWebText dataset. The script then creates a small validation split from the training data, as OpenWebText defaults to a 'train' split only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of workers in .map() call\n",
    "# good number to use is ~order number of cpu cores // 2\n",
    "num_proc = 8\n",
    "\n",
    "# number of workers in load_dataset() call\n",
    "# best number might be different from num_proc above as it also depends on NW speed.\n",
    "# it is better than 1 usually though\n",
    "num_proc_load_dataset = num_proc\n",
    "\n",
    "# takes 54GB in huggingface .cache dir, about 8M documents (8,013,769)\n",
    "dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)\n",
    "\n",
    "# owt by default only contains the 'train' split, so create a test split\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test') # rename the test split to val\n",
    "\n",
    "# this results in:\n",
    "# >>> split_dataset\n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['text'],\n",
    "#         num_rows: 8009762\n",
    "#     })\n",
    "#     val: Dataset({\n",
    "#         features: ['text'],\n",
    "#         num_rows: 4007\n",
    "#     })\n",
    "# })\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "### Tokenization Process (from documentation and script)\n",
    "Once the dataset is loaded and split, the text is tokenized. This script uses the GPT-2 Byte Pair Encoding (BPE) tokenizer provided by the `tiktoken` library.\n",
    "The `process` function in the script handles tokenization:\n",
    "1. It encodes the text into token IDs using `enc.encode_ordinary()`, which ignores special tokens.\n",
    "2. It appends an end-of-text (`eot_token`) to each document. For GPT-2 BPE, this token is `50256`.\n",
    "\n",
    "The `split_dataset.map()` function applies this tokenization process in parallel to all documents in the train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized = split_dataset.map(\n",
    "    process,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=num_proc,\n",
    ")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Format, Storage, and Writing to Binary Files\n",
    "\n",
    "### Data Format and Storage (from documentation)\n",
    "All datasets in nanoGPT are prepared and stored as arrays of integer token IDs in binary files:\n",
    "* Training data is stored in `train.bin`.\n",
    "* Validation data is stored in `val.bin`.\n",
    "* The data type is typically `np.uint16` because the maximum token ID in GPT-2's vocabulary (50256) is less than 2^16.\n",
    "\n",
    "During training, these binary files are memory-mapped for efficient access. This allows the system to train on datasets that might not fit entirely in RAM.\n",
    "\n",
    "### Writing to Files (from script)\n",
    "The script iterates through the tokenized train and validation sets. For each set:\n",
    "1. It calculates the total length of all token sequences (`arr_len`).\n",
    "2. It creates a memory-mapped NumPy array (`np.memmap`) with the appropriate filename (`train.bin` or `val.bin`), data type (`np.uint16`), and shape.\n",
    "3. It writes the token IDs into this memory-mapped array in batches for efficiency.\n",
    "4. Finally, `arr.flush()` ensures all data is written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "    filename = os.path.join(script_dir, f'{split}.bin') # Use script_dir defined earlier\n",
    "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()\n",
    "    print(f\"Finished writing {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Output and Statistics\n",
    "\n",
    "The script produces two binary files:\n",
    "* `train.bin`: Approximately 17GB, containing around 9 billion tokens.\n",
    "* `val.bin`: Approximately 8.5MB, containing around 4 million tokens.\n",
    "\n",
    "These files are now ready to be used by the `train.py` script in nanoGPT.\n",
    "\n",
    "### Reading the Binary Files (from script comments)\n",
    "To read these binary files later, for example with NumPy, you can use:\n",
    "```python\n",
    "# m = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Dataset Preparation (from documentation)\n",
    "\n",
    "The documentation also provides general steps for preparing a custom dataset for nanoGPT:\n",
    "1.  Load or download your text data.\n",
    "2.  Choose a tokenization method:\n",
    "    *   GPT-2 BPE tokenization (recommended for most cases).\n",
    "    *   Character-level tokenization (for smaller datasets or specific applications).\n",
    "3.  Tokenize the text and convert to integer IDs.\n",
    "4.  Split into training and validation sets.\n",
    "5.  Save as binary files using `numpy.tofile()` or memory-mapped arrays.\n",
    "\n",
    "The binary format allows nanoGPT's training process to efficiently load and process the data, regardless of dataset size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
